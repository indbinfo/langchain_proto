{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "with open('config.json') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "model_path = config['model_path']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 코드 생성 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "def code_generator(prompt, model_id, model_path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_id,\n",
    "        cache_dir=model_path,\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        cache_dir=model_path,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map='auto',\n",
    "    )\n",
    "\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "\n",
    "    generated = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=200,\n",
    "    )\n",
    "\n",
    "    output = tokenizer.decode(\n",
    "        generated[0][input_ids.shape[1]:], \n",
    "        skip_special_tokens=True\n",
    "        )\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a5650be3e0c44118262f89a0933e810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "file = open('/opt/sample_prompt/prompt1/input/code.txt')\n",
    "code = file.read()\n",
    "model_id = \"codellama/CodeLlama-7b-Python-hf\"\n",
    "\n",
    "output = code_generator(\n",
    "    prompt=code, \n",
    "    model_id=model_id, \n",
    "    model_path=model_path,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"\"\"\n",
      "\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "df = pd.read_csv(\"../input/credit-card-approval-prediction/creditcard.csv\")\n",
      "\n",
      "# 1. All code is written in Python.\n",
      "# 2. Write code to save the graph in the 'png' file in the 'test_graph' folder at the current location.\n",
      "# 3. Write code to save the description of the graph with a brief explanation. Save the file in the 'txt' format in the 'test_text' folder at the current location.\n",
      "# 4. There is no need to create a dataframe.\n",
      "# 5. The data is loaded and the dataframe name is df.\n",
      "\n",
      "# 1. All code is written in Python.\n",
      "# 2. Write code to save the graph in the 'png' file in the 'test_graph' folder at\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 한·영 번역"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "import torch\n",
    "\n",
    "class StoppingCriteriaSub(StoppingCriteria):\n",
    "    def __init__(self, stops = [], encounters=1):\n",
    "        super().__init__()\n",
    "        self.stops = [stop for stop in stops]\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        for stop in self.stops:\n",
    "            if torch.all((stop == input_ids[0][-len(stop):])).item():\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "stop_words_ids = torch.tensor([[829, 45107, 29958], [1533, 45107, 29958], [829, 45107, 29958], [21106, 45107, 29958]]).to(\"cuda\")\n",
    "stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_words_ids)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def translation_ko2en(lan, prompt, model_id, model_path, stopping_criteria):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map='auto',\n",
    "        cache_dir=model_path,\n",
    "        )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_id,\n",
    "        cache_dir=model_path,\n",
    "        )\n",
    "    \n",
    "    if (lan == 'ko'):\n",
    "        prompt_formatted = f\"### 한국어: {prompt}</끝>\\n### 영어:\"\n",
    "    elif (lan == 'en'):\n",
    "        prompt_formatted = f\"### 영어: {prompt}</끝>\\n### 한국어:\"\n",
    "        \n",
    "    generated = model.generate(\n",
    "        **tokenizer(\n",
    "            prompt_formatted,\n",
    "            return_tensors='pt',\n",
    "            return_token_type_ids=False\n",
    "            ).to('cuda'),\n",
    "            max_new_tokens=2000,\n",
    "            temperature=0.3,\n",
    "            num_beams=5,\n",
    "            stopping_criteria=stopping_criteria,\n",
    "            )\n",
    "    \n",
    "    output = tokenizer.decode(generated[0][1:]).replace(prompt_formatted+\" \", \"\").replace(\"</끝>\", \"\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('/opt/sample_prompt/prompt1/input/kr-eng.txt')\n",
    "kr_eng = file.read()\n",
    "model_id = \"squarelike/Gugugo-koen-7B-V1.1\" # 양방향 모델, 영어 번역 제한 가능한 것으로 보임\n",
    "\n",
    "output = translation_ko2en(\n",
    "    lan=\"ko\", \n",
    "    prompt=kr_eng[:2000], \n",
    "    model_id=model_id, \n",
    "    model_path=model_path, \n",
    "    stopping_criteria=stopping_criteria,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Requirements]\n",
      "Translate the contents from Text Start to Text End in English\n",
      "\n",
      "[Constraints]\n",
      "\n",
      "1.Translate only the contents from Text Start to Text End.\n",
      "\n",
      "2.\"\" enclosed parts keep the original value.\n",
      "\n",
      "3.The values of the data like \"부산 동래구\" keep the original value without translation.\n",
      "\n",
      "4.The description of the data related to the original data keep the original value as it is.\n",
      "\n",
      "5.Translate the entire contents of the text without shrinking the contents of the translation.\n",
      "\n",
      "[[Text Start]\n",
      "[context]\n",
      "The data is credit card data.\n",
      "The information about the data is as below.\n",
      "\n",
      " #   Column            Dtype  \n",
      "\n",
      "---  ------            -----\n",
      "\n",
      " 0   store_id          int64  \n",
      " 1   card_id           int64  \n",
      " 2   card_company      object \n",
      " 3   transacted_date   object \n",
      " 4   transacted_time   object \n",
      " 5   installment_term  int64  \n",
      " 6   region            object \n",
      " 7   type_of_business  object \n",
      " 8   amount            float64\n",
      "The additional information of each column is as below.\n",
      "The date column is in the format YYYY-MM-DD.\n",
      "The time column is in the format HH:MM.\n",
      "The values of the region column are as below.\n",
      "The array(['부산 동래구', '서울 종로구', '대구 수성구', '경기 용인시', '경기 안양시',\n",
      "       '서울 마포구', '부산 부산진구', '서울 중랑구', '서울 용산구', '전남 목포시', '서울 동작구',\n",
      "       '경북 경주시', '인천 계양구', '서울 강서구', '경기 성남시', '서울 영등포구', '서울 서대문구',\n",
      "       '서울 강남구', '서울 은평구', '서울 구로구', '서울 서초구', '서울 중구', '경기 시흥시',\n",
      "       '서울 양천구', '강원 강릉시', '서울 관악구', '인천 남구', '인천 동구', '충북 제천시', '인천 남동구',\n",
      "       '인천 연수구', '인천 부평구', '경기 화성시', '경기 평택시', '경기 안성시', '전남 순천시',\n",
      "       '경기 이천시', '경기 광주시', '경기 의정부시', '경기 포천시', '경기 양주시', '경기 파주시',\n",
      "       '경기 고양시', '서울 송파구', '충북 옥천군', '경기 부천시', '경기 광명시', '경기 남양주시',\n",
      "       '경기 안산시', '경기 오산시', '인천 서구', '경기 과천시', '경남 김해시', '경기 의왕시',\n",
      "       '제주 서귀포시', '전남 여수시', '제주 제주시', '부산 금정구', '경북 구미시', '광주 서구',\n",
      "       '충남 아산시', '부산 강서구', '경남 양산시', '경남 창원시', '강원 양구군', '충북 충주시',\n",
      "       '경남 통영시', '부산 사하구', '대구 남구', '경기 구리시', '강원 태백시', '서울 동대문구',\n",
      "       '충북 청주시', '대구 북구', '서울 성동구', '서울 광진구', '서울 성북구', '서울 강북구',\n",
      "       '서울 노원구', '서울 강동구', '강원 원주시', '서울 금천구', '부산 수영구', '부산 사상구',\n",
      "       '부산 북구', '강원 춘천시', '강원 삼척시', '전북 전주시', '강원 홍천군', '강원 횡성군', '광주 동구',\n",
      "       '강원 속초시', '경기 연천군', '경기 김포시', '부산 중구', '충북 단양군', '대전 서구', '울산 북구',\n",
      "       '경기 군포시', '경기 동두천시', '충남 서산시', '경북 영주시', '부산 남구', '충북 음성군',\n",
      "       '대전 대덕구', '대전 동구', '대전 중구', '충남 공주시', '충남 부여군', '경북 경산시', '충남 홍성군',\n",
      "       '충남 당진시',</s>\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 영·한 번역"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "def translation_en2ko(prompt, model_id, model_path):\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(\n",
    "        model_id,\n",
    "        cache_dir=model_path,\n",
    "    )\n",
    "\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        cache_dir=model_path,\n",
    "    )\n",
    "\n",
    "    tokenizer.pad_token_id = 2\n",
    "    tokenizer.eos_token = \"<|endoftext|>\"\n",
    "    tokenizer.eos_token_id = 46332\n",
    "    tokenizer.add_eos_token = True\n",
    "    tokenizer.padding_side = 'right'\n",
    "    tokenizer.model_max_length = 768\n",
    "\n",
    "    input_text = f\"### English: {prompt}\\n### 한국어: \"\n",
    "\n",
    "    inputs = tokenizer(\n",
    "    input_text, \n",
    "    return_tensors=\"pt\", \n",
    "    max_length=tokenizer.model_max_length, \n",
    "    truncation=True\n",
    "    )\n",
    "\n",
    "    inputs['input_ids'] = inputs['input_ids'][0][:-1].unsqueeze(dim=0)\n",
    "    inputs['attention_mask'] = inputs['attention_mask'][0][:-1].unsqueeze(dim=0)\n",
    "\n",
    "    generated = model.generate(\n",
    "        **inputs, \n",
    "        max_length=tokenizer.model_max_length, \n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    input_len = len(inputs['input_ids'].squeeze())\n",
    "    output = tokenizer.decode(\n",
    "        generated[0][input_len:], \n",
    "        skip_special_tokens=True\n",
    "        )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "733024b523de41d181867406b4f9709e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'[과제] [컨텍스트] 제공된 신용카드 데이터를 바탕으로 부산 동래구의 매출 상위 업종을 나타낸 막대그래프이다. 각 막대는 다른 업종을 나타내며, 막대의 높이는 해당 업종이 발생시킨 총매출액(원화, KRW)을 나타낸다. x축은 업종을 나타내며, y축은 총매출액을 나타낸다.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open('/opt/sample_prompt/prompt1/input/eng-kr.txt')\n",
    "eng_kr = file.read()\n",
    "model_id = \"traintogpb/llama-2-en2ko-translator-7b-qlora-bf16-upscaled\"\n",
    "\n",
    "translation_en2ko(prompt=eng_kr, model_id=model_id, model_path=model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
